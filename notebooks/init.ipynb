{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96cba85f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e244e6c2ba4b3e87f4c3c181e3bee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_pytorch_model.bin:   0%|          | 0.00/599M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0afe21a966c48c197985f31c9326c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_config.json:   0%|          | 0.00/469 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import open_clip\n",
    "\n",
    "model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:imageomics/bioclip')\n",
    "tokenizer = open_clip.get_tokenizer('hf-hub:imageomics/bioclip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b17a0761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "364be267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc4464ddddd74c1d95d0fc77f81e7070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_pytorch_model.bin:   0%|          | 0.00/784M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b5d938d8354e2cb5b685be38731c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_config.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4583c9cd5a2a4bbbae076086931f66ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a548e1e17a14dbcbc5d58293896c772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e22dafa70448f3a5b6e552a892eda8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/225k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from open_clip import create_model_from_pretrained, get_tokenizer # works on open-clip-torch>=2.23.0, timm>=0.9.8\n",
    "\n",
    "model, preprocess = create_model_from_pretrained('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "tokenizer = get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6152f47",
   "metadata": {},
   "source": [
    "## Single image BioCLIP prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de0905b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brain_MRI.jpg:\n",
      "brain MRI: 0.9999922513961792\n",
      "hematoxylin and eosin histopathology: 5.9478638831933495e-06\n",
      "immunohistochemistry histopathology: 1.6712749584257836e-06\n",
      "pie chart: 1.055264178262405e-07\n",
      "bone X-ray: 3.7441971301177546e-08\n",
      "chest X-ray: 4.858768054560869e-09\n",
      "adenocarcinoma histopathology: 1.9369059689466894e-09\n",
      "squamous cell carcinoma histopathology: 2.331514703524107e-10\n",
      "covid line chart: 3.6202614257102583e-12\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "\n",
    "template = 'this is a photo of '\n",
    "labels = [\n",
    "    'adenocarcinoma histopathology',\n",
    "    'brain MRI',\n",
    "    'covid line chart',\n",
    "    'squamous cell carcinoma histopathology',\n",
    "    'immunohistochemistry histopathology',\n",
    "    'bone X-ray',\n",
    "    'chest X-ray',\n",
    "    'pie chart',\n",
    "    'hematoxylin and eosin histopathology'\n",
    "]\n",
    "\n",
    "dataset_url = 'https://huggingface.co/microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224/resolve/main/example_data/biomed_image_classification_example_data/'\n",
    "test_imgs = ['brain_MRI.jpg']\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "context_length = 256\n",
    "\n",
    "images = torch.stack([preprocess(Image.open(urlopen(dataset_url + img))) for img in test_imgs]).to(device)\n",
    "texts = tokenizer([template + l for l in labels], context_length=context_length).to(device)\n",
    "with torch.no_grad():\n",
    "    image_features, text_features, logit_scale = model(images, texts)\n",
    "\n",
    "    logits = (logit_scale * image_features @ text_features.t()).detach().softmax(dim=-1)\n",
    "    sorted_indices = torch.argsort(logits, dim=-1, descending=True)\n",
    "\n",
    "    logits = logits.cpu().numpy()\n",
    "    sorted_indices = sorted_indices.cpu().numpy()\n",
    "\n",
    "top_k = -1\n",
    "\n",
    "for i, img in enumerate(test_imgs):\n",
    "    pred = labels[sorted_indices[i][0]]\n",
    "\n",
    "    top_k = len(labels) if top_k == -1 else top_k\n",
    "    print(img.split('/')[-1] + ':')\n",
    "    for j in range(top_k):\n",
    "        jth_index = sorted_indices[i][j]\n",
    "        print(f'{labels[jth_index]}: {logits[i][jth_index]}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac00b031",
   "metadata": {},
   "source": [
    "## Apply GEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c3029e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting gem_torch\n",
      "  Obtaining dependency information for gem_torch from https://files.pythonhosted.org/packages/4c/39/f362a75f13104011ce460fa553b79395edd6fdadedfd7c721454ba71a789/gem_torch-1.0.1-py3-none-any.whl.metadata\n",
      "  Downloading gem_torch-1.0.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: torch>=1.9.0 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from gem_torch) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from gem_torch) (0.15.2)\n",
      "Requirement already satisfied: regex in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from gem_torch) (2022.7.9)\n",
      "Requirement already satisfied: ftfy in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from gem_torch) (6.1.1)\n",
      "Requirement already satisfied: tqdm in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from gem_torch) (4.65.0)\n",
      "Requirement already satisfied: huggingface-hub in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from gem_torch) (0.16.4)\n",
      "Requirement already satisfied: sentencepiece in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from gem_torch) (0.2.0)\n",
      "Requirement already satisfied: protobuf in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from gem_torch) (5.27.0)\n",
      "Requirement already satisfied: timm in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from gem_torch) (0.9.7)\n",
      "Collecting einops (from gem_torch)\n",
      "  Obtaining dependency information for einops from https://files.pythonhosted.org/packages/44/5a/f0b9ad6c0a9017e62d4735daaeb11ba3b6c009d69a26141b258cd37b5588/einops-0.8.0-py3-none-any.whl.metadata\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: open-clip-torch in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from gem_torch) (2.24.0)\n",
      "Collecting opencv-python (from gem_torch)\n",
      "  Obtaining dependency information for opencv-python from https://files.pythonhosted.org/packages/d9/64/7fdfb9386511cd6805451e012c537073a79a958a58795c4e602e538c388c/opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: matplotlib in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from gem_torch) (3.7.2)\n",
      "Requirement already satisfied: numpy in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from gem_torch) (1.25.2)\n",
      "Requirement already satisfied: requests in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from gem_torch) (2.28.1)\n",
      "Requirement already satisfied: filelock in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from torch>=1.9.0->gem_torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from torch>=1.9.0->gem_torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from torch>=1.9.0->gem_torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from torch>=1.9.0->gem_torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from torch>=1.9.0->gem_torch) (3.1.2)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from ftfy->gem_torch) (0.2.5)\n",
      "Requirement already satisfied: fsspec in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from huggingface-hub->gem_torch) (2023.12.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from huggingface-hub->gem_torch) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from huggingface-hub->gem_torch) (22.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from matplotlib->gem_torch) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from matplotlib->gem_torch) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from matplotlib->gem_torch) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from matplotlib->gem_torch) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from matplotlib->gem_torch) (9.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from matplotlib->gem_torch) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from matplotlib->gem_torch) (2.8.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from requests->gem_torch) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from requests->gem_torch) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from requests->gem_torch) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from requests->gem_torch) (2023.7.22)\n",
      "Requirement already satisfied: safetensors in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from timm->gem_torch) (0.3.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->gem_torch) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from jinja2->torch>=1.9.0->gem_torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from sympy->torch>=1.9.0->gem_torch) (1.3.0)\n",
      "Downloading gem_torch-1.0.1-py3-none-any.whl (11 kB)\n",
      "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.2 MB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: meerkat-ml 0.2.5 has a non-standard dependency specifier multiprocess>=0.70.11Cython>=0.29.21. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of meerkat-ml or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: opencv-python, einops, gem_torch\n",
      "Successfully installed einops-0.8.0 gem_torch-1.0.1 opencv-python-4.9.0.80\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gem_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91c6b517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ViT-B/32            : openai\\nViT-B/32            : laion400m_e31\\nViT-B/32            : laion400m_e32\\nViT-B/32            : laion2b_e16\\nViT-B/32            : laion2b_s34b_b79k\\nViT-B/32-quickgelu  : metaclip_400m\\nViT-B/32-quickgelu  : metaclip_fullcc\\nViT-B/16            : openai\\nViT-B/16            : laion400m_e31\\nViT-B/16            : laion400m_e32\\nViT-B/16            : laion2b_s34b_b88k\\nViT-B/16-quickgelu  : metaclip_400m\\nViT-B/16-quickgelu  : metaclip_fullcc\\nViT-B/16-plus-240   : laion400m_e31\\nViT-B/16-plus-240   : laion400m_e32\\nViT-L/14            : openai\\nViT-L/14            : laion400m_e31\\nViT-L/14            : laion400m_e32\\nViT-L/14            : laion2b_s32b_b82k\\nViT-L/14-quickgelu  : metaclip_400m\\nViT-L/14-quickgelu  : metaclip_fullcc\\nViT-L/14-336        : openai\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gem\n",
    "gem.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "65058b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_clip import create_model_from_pretrained, get_tokenizer # works on open-clip-torch>=2.23.0, timm>=0.9.8\n",
    "\n",
    "model, preprocess = create_model_from_pretrained('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "tokenizer = get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6368ee7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomTextCLIP(\n",
       "  (visual): TimmModel(\n",
       "    (trunk): VisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (patch_drop): Identity()\n",
       "      (norm_pre): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (6): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (7): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (8): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (9): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (10): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (11): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (fc_norm): Identity()\n",
       "      (head_drop): Dropout(p=0.0, inplace=False)\n",
       "      (head): Identity()\n",
       "    )\n",
       "    (head): Sequential(\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=512, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (text): HFTextEncoder(\n",
       "    (transformer): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): ClsLastHiddenStatePooler()\n",
       "    (proj): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=640, bias=False)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=640, out_features=512, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2578498d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimmModel(\n",
       "  (trunk): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (patch_drop): Identity()\n",
       "    (norm_pre): Identity()\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (fc_norm): Identity()\n",
       "    (head_drop): Dropout(p=0.0, inplace=False)\n",
       "    (head): Identity()\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (proj): Linear(in_features=768, out_features=512, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.visual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e34af3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from open_clip.transformer import VisionTransformer\n",
    "\n",
    "from gem.gem_utils import SelfSelfAttention, GEMResidualBlock, modified_vit_forward\n",
    "\n",
    "\n",
    "class GEMWrapper(nn.Module):\n",
    "    def __init__(self, model, tokenizer, depth=7, ss_attn_iter=1, ss_attn_temp=None):\n",
    "        super(GEMWrapper, self).__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.depth = depth\n",
    "        self.ss_attn_iter = ss_attn_iter\n",
    "        self.ss_attn_temp = ss_attn_temp\n",
    "        self.patch_size = 16#self.model.visual.patch_size[0]\n",
    "        self.apply_gem()\n",
    "\n",
    "    def apply_gem(self):\n",
    "        for i in range(1, self.depth):\n",
    "            # Extract info from the original ViT\n",
    "            num_heads = self.model.visual.transformer.resblocks[-i].attn.num_heads\n",
    "            dim = int(self.model.visual.transformer.resblocks[-i].attn.head_dim * num_heads)\n",
    "            qkv_bias = True\n",
    "            # Init the self-self attention layer\n",
    "            ss_attn = SelfSelfAttention(dim=dim, num_heads=num_heads, qkv_bias=qkv_bias,\n",
    "                                        ss_attn_iter=self.ss_attn_iter, ss_attn_temp=self.ss_attn_temp)\n",
    "            # Copy necessary weights\n",
    "            ss_attn.qkv.weight.data = self.model.visual.transformer.resblocks[-i].attn.in_proj_weight.clone()\n",
    "            ss_attn.qkv.bias.data = self.model.visual.transformer.resblocks[-i].attn.in_proj_bias.clone()\n",
    "            ss_attn.proj.weight.data = self.model.visual.transformer.resblocks[-i].attn.out_proj.weight.clone()\n",
    "            ss_attn.proj.bias.data = self.model.visual.transformer.resblocks[-i].attn.out_proj.bias.clone()\n",
    "            # Swap the original Attention with our SelfSelfAttention\n",
    "            self.model.visual.transformer.resblocks[-i].attn = ss_attn\n",
    "            # Wrap Residual block to handle SelfSelfAttention outputs\n",
    "            self.model.visual.transformer.resblocks[-i] = GEMResidualBlock(self.model.visual.transformer.resblocks[-i])\n",
    "        # Modify ViT's forward function\n",
    "        self.model.visual.forward = modified_vit_forward.__get__(self.model.visual, VisionTransformer)\n",
    "        return\n",
    "\n",
    "    def encode_text(self, text: list):\n",
    "        prompts = [f'a photo of a {cls}.' for cls in text]\n",
    "        tokenized_prompts = self.tokenizer(prompts).to(self.model.visual.proj.device)\n",
    "        text_embedding = self.model.encode_text(tokenized_prompts)\n",
    "        text_embedding = F.normalize(text_embedding, dim=-1)\n",
    "        return text_embedding.unsqueeze(0)\n",
    "\n",
    "    def min_max(self, logits):\n",
    "        B, num_prompt = logits.shape[:2]\n",
    "        logits_min = logits.reshape(B, num_prompt, -1).min(dim=-1, keepdim=True)[0].unsqueeze(-1)\n",
    "        logits_max = logits.reshape(B, num_prompt, -1).max(dim=-1, keepdim=True)[0].unsqueeze(-1)\n",
    "        logits = (logits - logits_min) / (logits_max - logits_min)\n",
    "        return logits\n",
    "\n",
    "    def forward(self, image: torch.Tensor, text: list, normalize: bool = True, return_ori: bool =False):\n",
    "        \"\"\"\n",
    "        :param image: torch.Tensor [1, 3, H, W]\n",
    "        :param text: list[]\n",
    "        :param normalize: bool - if True performs min-max normalization\n",
    "        :param return_ori: bool - if True uses the features from the original visual encoder\n",
    "        \"\"\"\n",
    "        # Image\n",
    "        W, H = image.shape[-2:]\n",
    "        feat_gem, feat_ori = self.model.visual(image)\n",
    "        image_feat = feat_ori if return_ori else feat_gem\n",
    "        image_feat = F.normalize(image_feat, dim=-1)  # [1, N, dim]\n",
    "\n",
    "        # Text\n",
    "        text_embeddings = self.encode_text(text)  # [1, num_prompt, dim]\n",
    "\n",
    "        # Image-Text matching\n",
    "        img_txt_matching = image_feat[:, 1:] @ text_embeddings.transpose(-1, -2)  # [1, N, num_prompt]\n",
    "        img_txt_matching = rearrange(img_txt_matching, 'b (w h) c -> b c w h',\n",
    "                                     w=W//self.patch_size, h=H//self.patch_size)  # [1, num_prompt, w, h]\n",
    "\n",
    "        # Interpolate\n",
    "        img_txt_matching = F.interpolate(img_txt_matching, size=(W, H), mode='bilinear')  # [1, num_prompt, W, H]\n",
    "\n",
    "        # Heat Maps\n",
    "        if normalize:\n",
    "            img_txt_matching = self.min_max(img_txt_matching)\n",
    "        return img_txt_matching\n",
    "\n",
    "    def batched_forward(self, image: torch.Tensor, text: list, normalize: bool = True, return_ori: bool =False):\n",
    "        \"\"\"\n",
    "        :param image: torch.Tensor [B, 3, H, W]\n",
    "        :param text: list[list[]]\n",
    "        :param normalize: bool - if True performs min-max normalization\n",
    "        :param return_ori: bool - if True uses the features from the original visual encoder\n",
    "        \"\"\"\n",
    "        L = len(text)\n",
    "        cumm_idx = np.cumsum([len(t) for t in text]).tolist()\n",
    "        B, _, W, H = image.shape\n",
    "        assert B == L, f'Number of prompts L: {L} should be the same as number of images B: {B}.'\n",
    "\n",
    "        # Image\n",
    "        feat_gem, feat_ori = self.model.visual(image)\n",
    "        image_feat = feat_ori if return_ori else feat_gem\n",
    "        image_feat = F.normalize(image_feat, dim=-1)  # [B, N, dim]\n",
    "\n",
    "        # Text\n",
    "        flatten_text = [t for sub_text in text for t in sub_text]\n",
    "        text_embeddings = self.encode_text(flatten_text)  # [B, num_prompt, dim]\n",
    "\n",
    "        # Image-Text matching\n",
    "        img_txt_matching = 100 * image_feat[:, 1:] @ text_embeddings.transpose(-1, -2)  # [B, N, num_prompt]\n",
    "        img_txt_matching = rearrange(img_txt_matching, 'b (w h) c -> b c w h',\n",
    "                                     w=W // self.patch_size, h=H // self.patch_size)  # [B, num_prompt, w, h]\n",
    "\n",
    "        # Interpolate\n",
    "        img_txt_matching = F.interpolate(img_txt_matching, size=(W, H), mode='bilinear')  # [B,num_prompt, W, H]\n",
    "\n",
    "        # Heat Maps\n",
    "        if normalize:\n",
    "            img_txt_matching = self.min_max(img_txt_matching)  # [B,num_prompt, W, H]\n",
    "\n",
    "        # unflatten\n",
    "        img_txt_matching = torch.tensor_split(img_txt_matching, cumm_idx[:-1], dim=1)\n",
    "        img_txt_matching = [itm[i] for i, itm in enumerate(img_txt_matching)]\n",
    "        return img_txt_matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0f404081",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TimmModel' object has no attribute 'transformer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gem_model \u001b[38;5;241m=\u001b[39m \u001b[43mGEMWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[51], line 20\u001b[0m, in \u001b[0;36mGEMWrapper.__init__\u001b[0;34m(self, model, tokenizer, depth, ss_attn_iter, ss_attn_temp)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mss_attn_temp \u001b[38;5;241m=\u001b[39m ss_attn_temp\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\u001b[38;5;66;03m#self.model.visual.patch_size[0]\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[51], line 25\u001b[0m, in \u001b[0;36mGEMWrapper.apply_gem\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_gem\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth):\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;66;03m# Extract info from the original ViT\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m         num_heads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241m.\u001b[39mresblocks[\u001b[38;5;241m-\u001b[39mi]\u001b[38;5;241m.\u001b[39mattn\u001b[38;5;241m.\u001b[39mnum_heads\n\u001b[1;32m     26\u001b[0m         dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mvisual\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mresblocks[\u001b[38;5;241m-\u001b[39mi]\u001b[38;5;241m.\u001b[39mattn\u001b[38;5;241m.\u001b[39mhead_dim \u001b[38;5;241m*\u001b[39m num_heads)\n\u001b[1;32m     27\u001b[0m         qkv_bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/domino-generalized/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TimmModel' object has no attribute 'transformer'"
     ]
    }
   ],
   "source": [
    "gem_model = GEMWrapper(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d750e433",
   "metadata": {},
   "source": [
    "## LeGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2d29517f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting legrad_torch\n",
      "  Obtaining dependency information for legrad_torch from https://files.pythonhosted.org/packages/9f/a5/117d7d280926a76e48ddb36b40fb1e5b1b15dd1e0e2d25cac88180c57c07/legrad_torch-1.0-py3-none-any.whl.metadata\n",
      "  Downloading legrad_torch-1.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: torch>=1.11 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from legrad_torch) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from legrad_torch) (0.15.2)\n",
      "Requirement already satisfied: regex in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from legrad_torch) (2022.7.9)\n",
      "Requirement already satisfied: ftfy in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from legrad_torch) (6.1.1)\n",
      "Requirement already satisfied: tqdm in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from legrad_torch) (4.65.0)\n",
      "Requirement already satisfied: huggingface-hub in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from legrad_torch) (0.16.4)\n",
      "Requirement already satisfied: sentencepiece in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from legrad_torch) (0.2.0)\n",
      "Requirement already satisfied: protobuf in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from legrad_torch) (5.27.0)\n",
      "Requirement already satisfied: timm in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from legrad_torch) (0.9.7)\n",
      "Requirement already satisfied: einops in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from legrad_torch) (0.8.0)\n",
      "Requirement already satisfied: open-clip-torch in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from legrad_torch) (2.24.0)\n",
      "Requirement already satisfied: opencv-python in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from legrad_torch) (4.9.0.80)\n",
      "Requirement already satisfied: matplotlib in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from legrad_torch) (3.7.2)\n",
      "Requirement already satisfied: numpy in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from legrad_torch) (1.25.2)\n",
      "Requirement already satisfied: requests in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from legrad_torch) (2.28.1)\n",
      "Requirement already satisfied: filelock in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from torch>=1.11->legrad_torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from torch>=1.11->legrad_torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from torch>=1.11->legrad_torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from torch>=1.11->legrad_torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from torch>=1.11->legrad_torch) (3.1.2)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from ftfy->legrad_torch) (0.2.5)\n",
      "Requirement already satisfied: fsspec in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from huggingface-hub->legrad_torch) (2023.12.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from huggingface-hub->legrad_torch) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from huggingface-hub->legrad_torch) (22.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from matplotlib->legrad_torch) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from matplotlib->legrad_torch) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from matplotlib->legrad_torch) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from matplotlib->legrad_torch) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from matplotlib->legrad_torch) (9.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from matplotlib->legrad_torch) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from matplotlib->legrad_torch) (2.8.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from requests->legrad_torch) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from requests->legrad_torch) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from requests->legrad_torch) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from requests->legrad_torch) (2023.7.22)\n",
      "Requirement already satisfied: safetensors in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from timm->legrad_torch) (0.3.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->legrad_torch) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from jinja2->torch>=1.11->legrad_torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages (from sympy->torch>=1.11->legrad_torch) (1.3.0)\n",
      "Downloading legrad_torch-1.0-py3-none-any.whl (13 kB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/home/IAIS/rrao/anaconda3/envs/domino-generalized/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: meerkat-ml 0.2.5 has a non-standard dependency specifier multiprocess>=0.70.11Cython>=0.29.21. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of meerkat-ml or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: legrad_torch\n",
      "Successfully installed legrad_torch-1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install legrad_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "31f9ca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import legrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "12ecc0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "import torch\n",
    "\n",
    "from legrad import LeWrapper, LePreprocess\n",
    "from legrad.utils import visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "086a84b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_clip import create_model_from_pretrained, get_tokenizer # works on open-clip-torch>=2.23.0, timm>=0.9.8\n",
    "\n",
    "model, preprocess = create_model_from_pretrained('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "tokenizer = get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "84e9dd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f63a101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_clip import create_model_from_pretrained, get_tokenizer # works on open-clip-torch>=2.23.0, timm>=0.9.8\n",
    "\n",
    "model, preprocess = create_model_from_pretrained('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "tokenizer = get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aedc5b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activating necessary hooks and gradients ....\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'VisionTransformer' object has no attribute 'attn_pool'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLeWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/domino-generalized/lib/python3.10/site-packages/legrad/wrapper.py:34\u001b[0m, in \u001b[0;36mLeWrapper.__init__\u001b[0;34m(self, model, layer_index)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr, \u001b[38;5;28mgetattr\u001b[39m(model, attr))\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# ------------ activate hooks & gradient ------------\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_activate_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_index\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/domino-generalized/lib/python3.10/site-packages/legrad/wrapper.py:68\u001b[0m, in \u001b[0;36mLeWrapper._activate_hooks\u001b[0;34m(self, layer_index)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# --- Get starting depth (in case of negative layer_index) ---\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstarting_depth \u001b[38;5;241m=\u001b[39m layer_index \u001b[38;5;28;01mif\u001b[39;00m layer_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisual\u001b[38;5;241m.\u001b[39mtrunk\u001b[38;5;241m.\u001b[39mblocks) \u001b[38;5;241m+\u001b[39m layer_index\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_activate_timm_attn_pool_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel currently not supported, see legrad.list_pretrained() for a list of available models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/domino-generalized/lib/python3.10/site-packages/legrad/wrapper.py:134\u001b[0m, in \u001b[0;36mLeWrapper._activate_timm_attn_pool_hooks\u001b[0;34m(self, layer_index)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstarting_depth, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisual\u001b[38;5;241m.\u001b[39mtrunk\u001b[38;5;241m.\u001b[39mblocks)):\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisual\u001b[38;5;241m.\u001b[39mtrunk\u001b[38;5;241m.\u001b[39mblocks[layer]\u001b[38;5;241m.\u001b[39mforward \u001b[38;5;241m=\u001b[39m types\u001b[38;5;241m.\u001b[39mMethodType(hooked_resblock_timm_forward,\n\u001b[1;32m    131\u001b[0m                                                                \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisual\u001b[38;5;241m.\u001b[39mtrunk\u001b[38;5;241m.\u001b[39mblocks[layer])\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisual\u001b[38;5;241m.\u001b[39mtrunk\u001b[38;5;241m.\u001b[39mattn_pool\u001b[38;5;241m.\u001b[39mforward \u001b[38;5;241m=\u001b[39m types\u001b[38;5;241m.\u001b[39mMethodType(hooked_attentional_pooler_timm_forward,\n\u001b[0;32m--> 134\u001b[0m                                                        \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_pool\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/domino-generalized/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'VisionTransformer' object has no attribute 'attn_pool'"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model = LeWrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f520ede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
